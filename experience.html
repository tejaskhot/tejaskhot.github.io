<div class="row section_heading">
    <h2 ><a target="_blank" href="#experience">Experience</a></h2>
  
  <div class="row project_list">
    <div class="col-md-9">
      <h3 class="content_heading">Research Scholar, <a target="_blank" href="http://mlp.ece.vt.edu">Machine Learning and Perception Lab, Virginia Tech</a></h3>
        <p><i> Advisors : <a target="_blank" href="https://filebox.ece.vt.edu/~dbatra/">Dr. Dhruv Batra</a>, <a target="_blank" href="https://filebox.ece.vt.edu/~parikh/">Dr. Devi Parikh</a></i></p><br>
        <p class="content">I am working at the intersection of computer vision and natural language processing; specifically, applying deep learning to <a target="_blank" href="http://www.visualqa.org/">Visual Question Answering</a>. 
        <br><br>
        Joint first author - <b>Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</b><br>
        We propose to counter language priors in the dataset for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., ICCV 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs.<br>
        Our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair also provides a counter-example based explanation - specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.<br>
        <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>
        <a target="_blank" href="http://www.visualqa.org/vqa_v2.html">[Project Page] </a>
        <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a>
        <a target="_blank" href="">[Code coming soon!]</a>
        <br><br>
        Teaching Assistant - <b>(Advanced) Introduction to Machine Learning, Virginia Tech, Fall 2016</b><br>
        I was a Teaching Assistant for the (Advanced) Introduction to Machine Learning Course(Fall 2016: ECE 5424) with <a target="_blank" href="https://computing.ece.vt.edu/~steflee/">Dr. Stefan Lee</a>. I was responsible for setting up Kaggle competitions, curating homeworks and grading them.<br></p>
        <a target="_blank" href="https://filebox.ece.vt.edu/~f16ece5424/">[Course webpage]</a>
    </div>
    <div class="col-md-3" align="center">
      <img src="static/img/vt.jpg" width="150px" height="120px">
      <p>(July 2016 - Present)</p>
      <img src="static/img/vqa_logo.png" width="150px" height="120px">
    </div>
  </div>

  <div class="row project_list">
    <div class="col-md-9">
      <h3 class="content_heading">Research Intern, <a target="_blank" href="http://arl.fsktm.um.edu.my/">Advanced Robotic Lab, University of Malaya</a></h3>
        <p><i> Advisor : <a target="_blank" href="https://umexpert.um.edu.my/ckloo-um">Dr. Choo Kiong Loo</a></i></p><br>
        <p class="content">I worked on using <a href="http://minds.jacobs-university.de/conceptors">conceptor networks</a> to improve image
        classification performance on datasets like MNIST, CIFAR-10, CIFAR-100. I also used Stacked Convolutional Auto-encoders and
        spherical clustering, alongside hierarchical architectures like DeSTIN, for learning temporal dependencies in videos on
        datasets like KTH.</p>
    </div>
    <div class="col-md-3" align="center">
      <img src="static/img/um.png" width="150px" height="120px">
      <p>(June 2015 - July 2015)</p>
      <img src="static/img/conv_autoencoder.png" width="150px" height="120px">
    </div>
  </div>


</div>