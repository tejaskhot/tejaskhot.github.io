  <div class="row section_heading">
    <h2>Publications</h2>

    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">Learning Unsupervised Multi-View Stereopsis via Robust Photometric Consistency</h3>
        <p class="pubd">
          <span class="authors"><i><b>Tejas Khot*</b>, Shubham Agrawal*, Shubham Tulsiani, Christoph Mertz, Simon Lucey, Martial Hebert </i></span>
          <br><span class="conf"><a><font color='red'>Under Review</font></a></span>
        </p>

        <div class="col-xs-12">
          <!-- <a href="https://arxiv.org/abs/1808.00671">
            <img class="thumb" src="/static/img/thumb_unmvs.png" width="100%">
          </a> -->
          <a href="/static/docs/unmvs.pdf">
            <img class="thumb" src="/static/img/thumb_unmvs.png" width="80%" align="middle">
          </a>
        </div>

        <p class="content">We present a learning based approach for multi-view stereopsis (MVS). While current deep MVS methods achieve impressive results, they crucially rely on ground-truth 3D training data, and acquisition of such precise 3D geometry for supervision remains a major hurdle. We present a framework that instead leverages photometric consistency between multiple views as supervisory signal for learning depth prediction in a wide baseline MVS setup.  However, naively applying photo consistency constraints is undesirable due to occlusion and lighting changes across views. To overcome this, we propose a robust loss formulation that: a) enforces first order consistency and b) for each point, only forces consistency with some views, thereby implicitly handling occlusions during training. We demonstrate our ability to learn MVS without 3D supervision using a real dataset. We also ablate the various aspects of our proposed robust loss, and show that each component results in a significant improvement. We also qualitatively observe that reconstructions from our method are often more complete than the acquired ground truth, further showing the merits of this approach.<br>
        </p>

        <p>
          <!-- <a target="_blank" href="https://arxiv.org/abs/1808.00671" class="white-text"><button type="button" class="btn btn-primary ribbon">Arxiv</button></a>
          <a target="_blank" href="https://www.cs.cmu.edu/~wyuan1/pcn/" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
          <a target="_blank" href="https://www.youtube.com/watch?v=bzJrPQilPxg" class="white-text"><button type="button" class="btn btn-primary ribbon">YouTube Demo</button></a> -->
          <a target="_blank" href="/static/docs/unmvs.pdf" class="white-text"><button type="button" class="btn btn-primary ribbon">PDF</button></a>
        </p>
        <!-- <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>&nbsp -->
        <!-- <a target="_blank" href="http://www.visualqa.org">[Project Page] </a>&nbsp -->
        <!-- <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a> -->
      </div>
    </div>

    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">PCN: Point Completion Network</h3>
        <p class="pubd">
          <span class="authors"><i>Wentao Yuan, <b>Tejas Khot</b> , David Held, Christoph Mertz, Martial Hebert </i></span>
          <br><span class="conf"><a>3DV 2018 - <font color='red'>Oral, Honorable Mention for Best Paper Award</font></a></span>
        </p>

        <div class="col-xs-12">
          <a href="https://arxiv.org/abs/1808.00671">
            <img class="thumb" src="/static/img/pcn.png" width="50%%" align="middle">
          </a>
        </div>

        <p class="content">Shape completion, the problem of estimating the complete geometry of objects from partial observations lies at the core of many vision and robotics applications. In this work, we propose Point Completion Network (PCN), a novel learning-based approach for shape completion. While previous learning-based approaches use voxelized representations such as occupancy grids and distance fields, PCN directly takes a partial point cloud as input and generates a dense, complete point cloud without any voxelization. This enables PCN to produce completions of higher quality with fewer parameters. Without any prior structural assumptions (symmetry, planarity, etc) or additional annotations (part segmentation, class label, etc), our method works on inputs of various levels of incompleteness and is robust against noise. Trained on pairs of partial and complete point clouds from synthetic shapes, our model generalizes to novel incomplete shapes including cars from real LiDAR scans in KITTI, producing dense, complete point clouds with realistic structures in the missing regions. <br>
        </p>

        <p>
          <a target="_blank" href="https://arxiv.org/abs/1808.00671" class="white-text"><button type="button" class="btn btn-primary ribbon">Arxiv</button></a>
          <a target="_blank" href="https://www.cs.cmu.edu/~wyuan1/pcn/" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
          <a target="_blank" href="https://www.youtube.com/watch?v=bzJrPQilPxg" class="white-text"><button type="button" class="btn btn-primary ribbon">YouTube Demo</button></a>
        </p>
        <!-- <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>&nbsp -->
        <!-- <a target="_blank" href="http://www.visualqa.org">[Project Page] </a>&nbsp -->
        <!-- <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a> -->
      </div>
    </div>

    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</h3>
        <p class="pubd">
          <span class="authors"><i>Yash Goyal<sup>*</sup>, <b>Tejas Khot</b><sup>*</sup>, Douglas Summers-Stay, Dhruv Batra, Devi Parikh</i></span>
          <br><span class="conf"><a>CVPR 2017, IJCV 2018</a></span>
        </p>

        <!-- <p>
          <a href="https://visualqa.org" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
        </p> -->

        <div class="col-xs-12">
          <a href="https://arxiv.org/abs/1612.00837">
            <img class="thumb" src="/static/img/vqa.jpg" width="50%">
          </a>
        </div>

        <p class="content">We propose to counter language priors in the dataset for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., ICCV 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs.<br>
        Our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair also provides a counter-example based explanation - specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.<br>
        </p>

        <p>
          <a target="_blank" href="https://arxiv.org/abs/1612.00837" class="white-text"><button type="button" class="btn btn-primary ribbon">Arxiv</button></a>
          <a target="_blank" href="http://visualqa.org" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
          <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE" class="white-text"><button type="button" class="btn btn-primary ribbon">YouTube Demo</button></a>
        </p>
        <!-- <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>&nbsp -->
        <!-- <a target="_blank" href="http://www.visualqa.org">[Project Page] </a>&nbsp -->
        <!-- <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a> -->
      </div>
    </div>
  </div>
