  <div class="row section_heading">
    <h2>Publications</h2>

    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">PCN: Point Completion Network</h3>
        <p class="pubd">
          <span class="authors"><i>Wentao Yuan, <b>Tejas Khot</b> , David Held, Christoph Mertz, Martial Hebert </i></span>
          <br><span class="conf"><a>3DV 2018 - <font color='red'>Oral</font></a></span>
        </p>

        <div class="col-xs-12">
          <a href="https://arxiv.org/abs/1612.00837">
            <img class="thumb" src="/static/img/thumb_pcn.jpg" width="100%">
          </a>
        </div>

        <p class="content">Shape completion, the problem of estimating the complete geometry of objects from partial observations lies at the core of many vision and robotics applications. In this work, we propose Point Completion Network (PCN), a novel learning-based approach for shape completion. While previous learning-based approaches use voxelized representations such as occupancy grids and distance fields, PCN directly takes a partial point cloud as input and generates a dense, complete point cloud without any voxelization. This enables PCN to produce completions of higher quality with fewer parameters. Without any prior structural assumptions (symmetry, planarity, etc) or additional annotations (part segmentation, class label, etc), our method works on inputs of various levels of incompleteness and is robust against noise. Trained on pairs of partial and complete point clouds from synthetic shapes, our model generalizes to novel incomplete shapes including cars from real LiDAR scans in KITTI, producing dense, complete point clouds with realistic structures in the missing regions. <br>
        </p>

        <p>
          <a target="_blank" href="https://arxiv.org/abs/1808.00671" class="white-text"><button type="button" class="btn btn-primary ribbon">Arxiv</button></a>
          <a target="_blank" href="https://www.cs.cmu.edu/~wyuan1/pcn/" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
          <a target="_blank" href="https://www.youtube.com/watch?v=bzJrPQilPxg" class="white-text"><button type="button" class="btn btn-primary ribbon">YouTube Demo</button></a>
        </p>
        <!-- <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>&nbsp -->
        <!-- <a target="_blank" href="http://www.visualqa.org">[Project Page] </a>&nbsp -->
        <!-- <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a> -->
      </div>
    </div>

    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</h3>
        <p class="pubd">
          <span class="authors"><i>Yash Goyal<sup>*</sup>, <b>Tejas Khot</b><sup>*</sup>, Douglas Summers-Stay, Dhruv Batra, Devi Parikh</i></span>
          <br><span class="conf"><a>CVPR 2017 - Poster</a></span>
        </p>

        <!-- <p>
          <a href="https://visualqa.org" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
        </p> -->

        <div class="col-xs-12">
          <a href="https://arxiv.org/abs/1612.00837">
            <img class="thumb" src="/static/img/thumb_vqav2.jpg" width="100%">
          </a>
        </div>

        <p class="content">We propose to counter language priors in the dataset for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., ICCV 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs.<br>
        Our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair also provides a counter-example based explanation - specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.<br>
        </p>

        <p>
          <a target="_blank" href="https://arxiv.org/abs/1612.00837" class="white-text"><button type="button" class="btn btn-primary ribbon">Arxiv</button></a>
          <a target="_blank" href="http://visualqa.org" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
          <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE" class="white-text"><button type="button" class="btn btn-primary ribbon">YouTube Demo</button></a>
        </p>
        <!-- <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>&nbsp -->
        <!-- <a target="_blank" href="http://www.visualqa.org">[Project Page] </a>&nbsp -->
        <!-- <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a> -->
      </div>
    </div>
  </div>
